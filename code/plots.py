import matplotlib.pyplot as plt

run_pre = ""
run_fine = ""

jobs_pre = {

            "": ""}
jobs_fine = {"Sl_Ep_no-pre":"Run_16842006",
             "Sl_Ep":"Run_16842037",
             "": ""}




#### PRETRAINING ####
job_pre = jobs_pre[run_pre]
if job_pre == "":
    pass
elif job_pre == "":
    1

### FINE TUNING ###
job_fine = jobs_fine[run_fine]
if job_fine == "Run_16842006":
    Finetune_Accuracies= [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5166667103767395, 0.5166667103767395, 0.6666666865348816, 0.8000000715255737, 0.8333333730697632, 0.8166667222976685, 0.7833333611488342, 0.7833333611488342, 0.7333333492279053, 0.7333333492279053, 0.7333333492279053, 0.6833333969116211, 0.7166666984558105, 0.6833333969116211, 0.7333333492279053, 0.7000000476837158, 0.7000000476837158, 0.6833333969116211, 0.7000000476837158, 0.7166666984558105, 0.6833333969116211, 0.6833333969116211, 0.7000000476837158, 0.7166666984558105, 0.6833333969116211, 0.6666666865348816, 0.7166666984558105]
    Finetune_Losses= [5.494187355041504, 5.510456562042236, 5.063237190246582, 5.295530319213867, 5.206552505493164, 5.228387832641602, 5.232378005981445, 5.35379695892334, 5.506148815155029, 5.3091936111450195, 5.379264831542969, 5.280365467071533, 5.2264604568481445, 4.985601425170898, 5.029535293579102, 5.237789154052734, 5.273306846618652, 5.249920845031738, 5.311746597290039, 5.272881507873535, 5.184634685516357, 5.088433265686035, 5.3404765129089355, 4.881678581237793, 5.437016487121582, 5.500899791717529, 5.57672643661499, 5.158933639526367, 4.961869239807129, 5.414020538330078, 5.157688140869141, 5.046280860900879, 4.775415420532227, 5.054806709289551, 4.76085090637207, 4.556619644165039, 5.1791863441467285, 5.066056251525879, 5.103964328765869, 5.045329570770264]
    Test_Accuracies= [0.19877836108207703, 0.19808028638362885, 0.19808028638362885, 0.19808028638362885, 0.19808028638362885, 0.1979057490825653, 0.1979057490825653, 0.1979057788848877, 0.19738222658634186, 0.1982548087835312, 0.19773125648498535, 0.1982548087835312, 0.19773125648498535, 0.19773124158382416, 0.19799301028251648, 0.8291448354721069, 0.8107330203056335, 0.8062826991081238, 0.8049739003181458, 0.8066317439079285, 0.8079407215118408, 0.8116056323051453, 0.816055953502655, 0.8220768570899963, 0.8273125290870667, 0.8335078954696655, 0.8389179706573486, 0.8440663814544678, 0.849127471446991, 0.8538395166397095, 0.8574171662330627, 0.8615183234214783, 0.8639616370201111, 0.8678011298179626, 0.8723386526107788, 0.8765270709991455, 0.8802793025970459, 0.8847295045852661, 0.8885688781738281, 0.8926701545715332]
    Test_Losses= [0.9000720977783203, 0.8887413144111633, 0.876979649066925, 0.8647187948226929, 0.8519177436828613, 0.8384384512901306, 0.8245747089385986, 0.8098180294036865, 0.7942797541618347, 0.7778317332267761, 0.7612512111663818, 0.7446260452270508, 0.7276453971862793, 0.7104031443595886, 0.6937870383262634, 0.6773563623428345, 0.6616872549057007, 0.6456319093704224, 0.6296148896217346, 0.6148850321769714, 0.6007936596870422, 0.5877537131309509, 0.5741788744926453, 0.5618709921836853, 0.5500906705856323, 0.5375406742095947, 0.5252520442008972, 0.5135241150856018, 0.5010573863983154, 0.49058473110198975, 0.4803737699985504, 0.4696926176548004, 0.4594990909099579, 0.44952496886253357, 0.4397423565387726, 0.4311448633670807, 0.42188993096351624, 0.413677841424942, 0.40658894181251526, 0.39903146028518677]
elif job_fine == "Run_16842037":
    Finetune_Accuracies= [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5166667103767395, 0.5166667103767395, 0.5333333611488342, 0.6000000238418579, 0.6500000357627869, 0.6833333969116211, 0.7000000476837158, 0.8000000715255737, 0.8000000715255737, 0.7166666984558105, 0.7000000476837158, 0.6833333969116211, 0.7166666984558105, 0.6833333969116211, 0.6500000357627869, 0.6500000357627869]
    Finetune_Losses= [3.608126640319824, 8.49444580078125, 5.670309066772461, 5.83503532409668, 5.406331539154053, 5.483116626739502, 4.603682518005371, 5.190855979919434, 5.004469394683838, 5.09213399887085, 5.362924098968506, 5.368458271026611, 4.652036190032959, 6.089414596557617, 6.523703575134277, 4.244708061218262, 4.989120006561279, 4.823812961578369, 3.3343141078948975, 5.2811784744262695, 5.098235130310059, 5.559891223907471, 5.814483642578125, 4.777490615844727, 5.124426364898682, 4.8054022789001465, 5.545544624328613, 6.65397834777832, 4.927772521972656, 5.173673152923584, 4.916828632354736, 5.045337200164795, 4.219543933868408, 3.754121780395508, 6.688282012939453, 4.846285343170166, 4.736876487731934, 2.9305498600006104, 6.476268768310547, 5.544727325439453]
    Test_Accuracies= [0.19877836108207703, 0.19808028638362885, 0.19808028638362885, 0.19808028638362885, 0.19808028638362885, 0.1979057490825653, 0.1979057490825653, 0.1979057788848877, 0.19738222658634186, 0.1982548087835312, 0.19773125648498535, 0.1982548087835312, 0.19773125648498535, 0.19773124158382416, 0.19808028638362885, 0.197556734085083, 0.19773125648498535, 0.19773125648498535, 0.19808028638362885, 0.197556734085083, 0.19773125648498535, 0.19773124158382416, 0.1979057788848877, 0.19755671918392181, 0.19808028638362885, 0.19755671918392181, 0.19755671918392181, 0.19773125648498535, 0.197556734085083, 0.1979057788848877, 0.19808028638362885, 0.20017454028129578, 0.23429320752620697, 0.8783595561981201, 0.9104712009429932, 0.9075915813446045, 0.9131762981414795, 0.9232984781265259, 0.9314136505126953, 0.9389180541038513]
    Test_Losses= [0.8977836966514587, 0.8905176520347595, 0.8823469281196594, 0.8739126920700073, 0.8664315938949585, 0.8586897850036621, 0.8513829112052917, 0.8453241586685181, 0.8403059840202332, 0.8355948328971863, 0.8315396308898926, 0.8266355395317078, 0.8214184641838074, 0.8147369027137756, 0.8081582188606262, 0.8005658984184265, 0.7908445596694946, 0.7829086780548096, 0.7745748162269592, 0.7664965987205505, 0.7561615109443665, 0.7473430633544922, 0.737797200679779, 0.7347555756568909, 0.7250185608863831, 0.7181878089904785, 0.7096663117408752, 0.7071741819381714, 0.7055939435958862, 0.7021913528442383, 0.6982327699661255, 0.6944097280502319, 0.6916969418525696, 0.6833479404449463, 0.6741426587104797, 0.6654331088066101, 0.6576157212257385, 0.653618574142456, 0.6476786136627197, 0.6449494957923889]



#### PLOTTING PRE TRAINING ####
if run_pre != "":
    # Plot data on each subplot
    fig, axs = plt.subplots(2, 3, figsize=(8, 5))
    axs[0, 0].plot(loss)
    axs[0, 1].plot(loss_t)
    axs[1, 0].plot(loss_f)
    axs[1, 1].plot(loss_c)
    axs[1, 2].plot(loss_TF)

    axs[0, 0].set_title('Average loss')
    axs[0, 1].set_title('Average loss time')
    axs[1, 0].set_title('Average loss frequency')
    axs[1, 1].set_title('Average loss c')
    axs[1, 2].set_title('Average loss TF')

    plt.tight_layout()
    plt.suptitle(f"Pre-training, {run_pre}")
    plt.subplots_adjust(top=0.88)
    plt.show()


##### PLOTTING FINE TUNE #####
if run_fine != "":
    fig, axs = plt.subplots(2, 2, figsize=(7, 6))
    axs[0, 0].plot(Finetune_Accuracies)
    axs[0, 1].plot(Finetune_Losses)
    axs[1, 0].plot(Test_Accuracies)
    axs[1, 1].plot(Test_Losses)

    axs[0, 0].set_title('Finetune accuracy')
    axs[0, 1].set_title('Finetune loss')
    axs[1, 0].set_title('Test accuracy')
    axs[1, 1].set_title('Test loss')

    plt.tight_layout()
    plt.suptitle(f"Fine-tuning, {run_fine}")
    plt.subplots_adjust(top=0.88)
    plt.show()